{
  "config": {
    "lang": [
      "en"
    ],
    "separator": "[\\s\\-]+",
    "pipeline": [
      "stopWordFilter"
    ],
    "fields": {
      "title": {
        "boost": 1000.0
      },
      "text": {
        "boost": 1.0
      },
      "tags": {
        "boost": 1000000.0
      }
    }
  },
  "docs": [
    {
      "location": "",
      "title": "batchling",
      "text": "<p>Save 50% off GenAI costs in two lines of code.</p> <p>batchling repurposes incoming GenAI requests to batch format, gets results from supported Batch APIs and   returns responses seamlessly in your code execution, reducing GenAI costs at scale with zero friction.</p> Quickstart Install and run the first async batching flow in minutes. Frameworks &amp; Providers See the list of supported frameworks and providers Usage Learn how to use batchling Advanced Features Learn about advanced features: caching, dry run, deferred mode.. <p> Save 50% off GenAI costs in two lines of code </p> <p> <p>batchling is a frictionless, batteries-included plugin to convert any GenAI async function or script into half-cost deferred jobs.</p> <p>Key features:</p> <ul> <li>Simplicity: a simple 2-liner gets you 50% off your GenAI bill instantly.</li> <li>Transparent: Your code remains the same, no added behaviors. Track sent batches easily.</li> <li>Global: Integrates with most providers and all frameworks.</li> <li>Safe: Get a complete breakdown of your cost savings before launching a single batch.</li> <li>Lightweight: Very few dependencies.</li> </ul> What's the catch? <p>The batch is the catch!</p> <p>Batch APIs enable you to process large volumes of requests asynchronously (usually at 50% lower cost compared to real-time API calls). It's perfect for workloads that don't need immediate responses such as:</p> <ul> <li>Running mass offline evaluations</li> <li>Classifying large datasets</li> <li>Generating large-scale embeddings</li> <li>Offline summarization</li> <li>Synthetic data generation</li> <li>Structured data extraction (e.g. OCR)</li> <li>Audio transcriptions/translations at scale</li> </ul> <p>Compared to using standard endpoints directly, Batch API offers:</p> <ul> <li>Better cost efficiency: usually 50% cost discount compared to synchronous APIs</li> <li>Higher rate limits: Substantially more headroom with separate rate limit pools</li> <li>Large-scale support: Process thousands of requests per batch</li> <li>Flexible completion: Best-effort completion within 24 hours with progress tracking, batches usually complete within an hour.</li> </ul>"
    },
    {
      "location": "#installation",
      "title": "Installation",
      "text": "<pre><code>pip install batchling\n</code></pre>"
    },
    {
      "location": "#get-started",
      "title": "Get Started",
      "text": ""
    },
    {
      "location": "#using-the-async-context-manager-recommended",
      "title": "Using the async context manager (recommended)",
      "text": "<pre><code>import asyncio\nfrom batchling import batchify\nfrom openai import AsyncOpenAI\n\nasync def generate():\n    client = AsyncOpenAI()\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    tasks = [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n    with batchify(): # Runs your tasks as batches, save 50%\n        responses = await asyncio.gather(*tasks)\n</code></pre>"
    },
    {
      "location": "#using-the-cli-wrapper",
      "title": "Using the CLI wrapper",
      "text": "<p>Create a file <code>main.py</code> with:</p> <pre><code>import asyncio\nfrom openai import AsyncOpenAI\n\nasync def generate():\n    client = AsyncOpenAI()\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    tasks = [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n    responses = await asyncio.gather(*tasks)\n</code></pre> <p>Run your function in batch mode:</p> <pre><code>batchling main.py:generate\n</code></pre>"
    },
    {
      "location": "#how-it-works",
      "title": "How it works",
      "text": ""
    },
    {
      "location": "#request-interception",
      "title": "Request interception",
      "text": "<p>batchling patches <code>httpx</code> and <code>aiohttp</code>, the two main async request utilities to capture certain requests based on batchable endpoint detection for GenAI providers exposing a batch API.</p>"
    },
    {
      "location": "#batch-management",
      "title": "Batch Management",
      "text": "<p>Relevant requests are routed to a batch manager, which orchestrates batch queues. Batch queues behaviors are parametrized by user-defined parameters <code>batch_size</code> and <code>batch_window_size</code>. Each batch queue is associated with a unique <code>(provider, endpoint, model)</code> key.</p>"
    },
    {
      "location": "#requests-repurposing",
      "title": "Requests repurposing",
      "text": "<p>Once batches are ready to go, the received requests are repurposed into batch-compatible requests and sent to the provider. Batches are automatically polled every <code>batch_poll_interval_seconds</code> seconds. Once all batches have been processed, the data is returned through the <code>httpx</code> or <code>aiohttp</code> patch like a regular request would have been, letting the rest of the script execute.</p>"
    },
    {
      "location": "#request-cache-and-deferred-mode",
      "title": "Request cache and deferred mode",
      "text": "<ul> <li>Request cache is enabled by default and can be disabled with <code>--no-cache</code> or <code>cache=False</code>.</li> <li>Cache entries are cleaned with a one-month retention window.</li> <li>Deferred mode (<code>--deferred</code> or <code>deferred=True</code>) can stop runtime when only polling remains idle.</li> <li>CLI mode exits successfully with a deferred message; Python usage raises <code>DeferredExit</code>.</li> </ul>"
    },
    {
      "location": "#supported-providers",
      "title": "Supported providers",
      "text": "Name Batch API Docs URL OpenAI https://platform.openai.com/docs/guides/batch Anthropic https://docs.anthropic.com/en/docs/build-with-claude/batch-processing Gemini https://ai.google.dev/gemini-api/docs/batch-mode Groq https://console.groq.com/docs/batch Mistral https://docs.mistral.ai/capabilities/batch/ Together AI https://docs.together.ai/docs/batch-inference Doubleword https://docs.doubleword.ai/batches/getting-started-with-batched-api"
    },
    {
      "location": "advanced-features/",
      "title": "Advanced Features",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "batchify/",
      "title": "batchify",
      "text": "<p>The <code>batchify</code> function is the one and only entrypoint for the <code>batchling</code> library, everything starts from there.</p> <p>It can be used either through the Python SDK or CLI depending on your needs.</p> <p>It comes with a bunch of parameters that you can customize to alter how batching is performed:</p>"
    },
    {
      "location": "batchify/#batchling.api.batchify",
      "title": "batchify",
      "text": "<pre><code>batchify(\n    batch_size=50,\n    batch_window_seconds=2.0,\n    batch_poll_interval_seconds=10.0,\n    dry_run=False,\n    cache=True,\n    deferred=False,\n    deferred_idle_seconds=60.0,\n)\n</code></pre> <p>Context manager used to activate batching for a scoped context. Requests are accumulated within this context and will be batched and sent to the provider when the batch size or window is reached. Batches are accumulated in different queues based on the provider/endpoint/model triplet.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Submit a batch when this many requests are queued for a given provider/endpoint/model triplet.</p> <code>50</code> <code>batch_window_seconds</code> <code>float</code> <p>Submit a provider batch after this many seconds from the moment the first request is queued, even if size not reached.</p> <code>2.0</code> <code>batch_poll_interval_seconds</code> <code>float</code> <p>Poll active batches for results every this many seconds.</p> <code>10.0</code> <code>dry_run</code> <code>bool</code> <p>If <code>True</code>, intercept and batch requests without sending provider batches. Use it to debug or before sending big jobs. Batched requests resolve to synthetic responses.</p> <code>False</code> <code>cache</code> <code>bool</code> <p>If <code>True</code>, enable persistent request cache lookups. This parameter allows to skip the batch submission and go straight to the polling phase for requests that have already been sent.</p> <code>True</code> <code>deferred</code> <code>bool</code> <p>If <code>True</code>, allow deferred-mode idle termination while polling. Deferred mode exits the program once polling is idle for some time. Useful to avoid having an async long-running process and process batches the deferred way.</p> <code>False</code> <code>deferred_idle_seconds</code> <code>float</code> <p>Idle threshold before deferred mode triggers a controlled early exit. Useful to avoid having an async long-running process and process batches the deferred way.</p> <code>60.0</code> <p>Returns:</p> Type Description <code>BatchingContext</code> <p>Context manager that yields <code>None</code>. The context manager is only used to scope the batching context and is not used to yield any target.</p> Source code in <code>src/batchling/api.py</code> <pre><code>def batchify(\n    batch_size: int = 50,\n    batch_window_seconds: float = 2.0,\n    batch_poll_interval_seconds: float = 10.0,\n    dry_run: bool = False,\n    cache: bool = True,\n    deferred: bool = False,\n    deferred_idle_seconds: float = 60.0,\n) -&gt; BatchingContext:\n    \"\"\"\n    Context manager used to activate batching for a scoped context.&lt;br&gt;\n    Requests are accumulated within this context and will be batched and sent to the provider when the batch size or window is reached.&lt;br&gt;\n    Batches are accumulated in different queues based on the provider/endpoint/model triplet.\n\n    Parameters\n    ----------\n    batch_size : int, optional\n        Submit a batch when this many requests are queued for a given provider/endpoint/model triplet.\n    batch_window_seconds : float, optional\n        Submit a provider batch after this many seconds from the moment the first request is queued, even if size not reached.\n    batch_poll_interval_seconds : float, optional\n        Poll active batches for results every this many seconds.\n    dry_run : bool, optional\n        If ``True``, intercept and batch requests without sending provider batches.&lt;br&gt;\n        Use it to debug or before sending big jobs.&lt;br&gt;\n        Batched requests resolve to synthetic responses.\n    cache : bool, optional\n        If ``True``, enable persistent request cache lookups.&lt;br&gt;\n        This parameter allows to skip the batch submission and go straight to the polling phase for requests that have already been sent.\n    deferred : bool, optional\n        If ``True``, allow deferred-mode idle termination while polling.&lt;br&gt;\n        Deferred mode exits the program once polling is idle for some time.&lt;br&gt;\n        Useful to avoid having an async long-running process and process batches the deferred way.\n    deferred_idle_seconds : float, optional\n        Idle threshold before deferred mode triggers a controlled early exit.&lt;br&gt;\n        Useful to avoid having an async long-running process and process batches the deferred way.\n\n    Returns\n    -------\n    BatchingContext\n        Context manager that yields ``None``.&lt;br&gt;\n        The context manager is only used to scope the batching context and is not used to yield any target.\n    \"\"\"\n    # 1. Install hooks globally (idempotent)\n    install_hooks()\n\n    # 2. Create Batcher instance with provided configuration\n    batcher = Batcher(\n        batch_size=batch_size,\n        batch_window_seconds=batch_window_seconds,\n        batch_poll_interval_seconds=batch_poll_interval_seconds,\n        dry_run=dry_run,\n        cache=cache,\n        deferred=deferred,\n        deferred_idle_seconds=deferred_idle_seconds,\n    )\n\n    # 3. Return BatchingContext with no yielded target.\n    return BatchingContext(\n        batcher=batcher,\n    )\n</code></pre>"
    },
    {
      "location": "batchify/#next-steps",
      "title": "Next steps",
      "text": "<p>Now that you have more information about the <code>batchify</code> function, you can:</p> <ul> <li> <p>Learn to use it from the Python SDK or the CLI</p> </li> <li> <p>See on which Frameworks &amp; Providers it can be used</p> </li> <li> <p>Consult in-depth use-cases leveraging <code>batchify</code></p> </li> <li> <p>Learn more about advanced features</p> </li> </ul>"
    },
    {
      "location": "cache/",
      "title": "Cache",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "cli/",
      "title": "CLI",
      "text": "<p>batchling comes with a CLI included that further facilitates batching an existing process.</p> <p>The CLI pushes the abstraction even more, to a point where you don't even need to modify your code to start batching.</p>"
    },
    {
      "location": "cli/#example",
      "title": "Example",
      "text": "<p>Suppose you work in e-commerce and you have a cutting-edge business which uses GenAI to generate product visuals whenever you receive new products from your vendors. You have a function <code>generate_image</code> that takes a prompt (string) as input and generates the corresponding image using an API, returning image bytes. You would typically run concurrent tasks using <code>asyncio</code> for tasks not blocking each other in the execution loop.</p> <pre><code># generate_product_images.py\nimport asyncio\n\nasync def get_new_product_descriptions() -&gt; list[str]:\n    \"\"\"\n    Fetches pending product descriptions having no image yet\n    \"\"\"\n    ...\n\nasync def generate_image(text: str) -&gt; bytes:\n    ...\n\nasync def update_product_db(generated_images: list[bytes]):\n    \"\"\"\n    Update product db with generated images for review\n    \"\"\"\n\nasync def main():\n    new_product_descriptions = await get_new_product_descriptions()\n    tasks = [\n        generate_image(text=product_description)\n        for product_description in new_product_descriptions\n    ]\n    generated_images = asyncio.gather(*tasks)\n    await update_product_db(generated_images=generated_images)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This approach might have several drawbacks for you:</p> <ol> <li>You pay the full price for calls that likely can wait at most 24h.</li> <li>You might hit your providers' rate limits if you send all products at once, so you likely need to add more complex structures like semaphores.</li> <li>Even with semaphores, you might run into daily rate limits, so you likely want to implement retries with exponential waiting</li> </ol> <p>The batch inference approach solves all those issues:</p> <ol> <li>Cut costs by 50% if you can wait up to 24 hours.</li> <li>Batch APIs rate limits are made for high-volume and thus always higher (near-infinite for certain providers)</li> <li>No retries mechanism needed, you keep the code simple and light.</li> </ol> <p>Here's how you can leverage the <code>batchling</code> CLI to transform that workflow from async to batched.</p> <p>In your terminal, run the following command:</p> <pre><code>batchling generate_product_images.py:main\n</code></pre> <p>That's it! Just run that command and you save 50% off your workflow.</p>"
    },
    {
      "location": "cli/#next-steps",
      "title": "Next Steps",
      "text": "<p>If you haven't yet, look at how you can:</p> <ul> <li> <p>leverage the Python SDK to scope batching to specific portions of your code.</p> </li> <li> <p>use <code>batchling</code> with any provider / framework with examples</p> </li> <li> <p>learn about end-to-end use-cases using <code>batchling</code></p> </li> <li> <p>learn about advanced features</p> </li> </ul>"
    },
    {
      "location": "deferred-mode/",
      "title": "Deferred Mode",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "dry-run/",
      "title": "Dry Run",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "frameworks/",
      "title": "Frameworks",
      "text": "<p>Since it operates at the network level by intercepting select async GenAI requests, <code>batchling</code> is natively compatible with all frameworks using <code>httpx</code> or <code>aiohttp</code> as their async request engine.</p> <p>Below are the frameworks we tested that we are sure are compatible with <code>batchling</code>:</p> <ul> <li>LangChain</li> <li>Pydantic AI</li> </ul>"
    },
    {
      "location": "how-it-works/",
      "title": "How it works",
      "text": "<p>Placeholder content for the internal flow explanation.</p>"
    },
    {
      "location": "providers/",
      "title": "Providers",
      "text": "<p><code>batchling</code> is compatible with most providers exposing a Batch API. The following providers are supported by <code>batchling</code>:</p> <ul> <li>Anthropic</li> <li>Doubleword</li> <li>Gemini</li> <li>Groq</li> <li>Mistral</li> <li>OpenAI</li> <li>Together</li> </ul>"
    },
    {
      "location": "python-sdk/",
      "title": "Python SDK",
      "text": "<p>To use <code>batchling</code>, you only need to learn to use one function exposed through the library: <code>batchify</code></p> <p>The <code>batchify</code> function is meant to be used as a context manager wrapping portions of your code containing GenAI calls that you want to batch.</p>"
    },
    {
      "location": "python-sdk/#example",
      "title": "Example",
      "text": "<p>Suppose you work in e-commerce and you have a cutting-edge business which uses GenAI to generate product visuals whenever you receive new products from your vendors. You have a function <code>generate_image</code> that takes a prompt (string) as input and generates the corresponding image using an API, returning image bytes. You would typically run concurrent tasks using <code>asyncio</code> for tasks not blocking each other in the execution loop.</p> <pre><code>import asyncio\n\nasync def get_new_product_descriptions() -&gt; list[str]:\n    \"\"\"\n    Fetches pending product descriptions having no image yet\n    \"\"\"\n    ...\n\nasync def generate_image(text: str) -&gt; bytes:\n    ...\n\nasync def update_product_db(generated_images: list[bytes]):\n    \"\"\"\n    Update product db with generated images for review\n    \"\"\"\n\nasync def main():\n    new_product_descriptions = await get_new_product_descriptions()\n    tasks = [\n        generate_image(text=product_description)\n        for product_description in new_product_descriptions\n    ]\n    generated_images = asyncio.gather(*tasks)\n    await update_product_db(generated_images=generated_images)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This approach might have several drawbacks for you:</p> <ol> <li>You pay the full price for calls that likely can wait at most 24h.</li> <li>You might hit your providers' rate limits if you send all products at once, so you likely need to add more complex structures like semaphores.</li> <li>Even with semaphores, you might run into daily rate limits, so you likely want to implement retries with exponential waiting</li> </ol> <p>The batch inference approach solves all those issues:</p> <ol> <li>Cut costs by 50% if you can wait up to 24 hours.</li> <li>Batch APIs rate limits are made for high-volume and thus always higher (near-infinite for certain providers)</li> <li>No retries mechanism needed, you keep the code simple and light.</li> </ol> <p>Here's how you can leverage <code>batchling</code> Python SDK to transform that workflow from async to batched:</p> <p>Add one import at the top of your file:</p> <pre><code>+ from batchling import batchify\n</code></pre> <p>Add the context manager scoping to your script:</p> <pre><code>async def main():\n    new_product_descriptions = await get_new_product_descriptions()\n    tasks = [\n        generate_image(text=product_description)\n        for product_description in new_product_descriptions\n    ]\n-   generated_images = asyncio.gather(*tasks)\n+   with batchify():\n+       generated_images = asyncio.gather(*tasks)\n    await update_product_db(generated_images=generated_images)\n</code></pre> <p>That's it! Update three lines of code and you save 50% off your workflow.</p>"
    },
    {
      "location": "python-sdk/#next-steps",
      "title": "Next Steps",
      "text": "<p>If you haven't yet, look at how you can:</p> <ul> <li> <p>leverage the CLI to batchify full scripts in one command</p> </li> <li> <p>use <code>batchling</code> with any provider / framework with examples</p> </li> <li> <p>learn about end-to-end use-cases using <code>batchling</code></p> </li> <li> <p>learn about advanced features</p> </li> </ul>"
    },
    {
      "location": "quickstart/",
      "title": "Quickstart",
      "text": "<p>batchling is a powerful python library that lets you transform any async GenAI workflow to batched inference, saving you money in exchange of deferred execution (24 hours bound). You typically want to use batched inference when running any process that does not require immediate response and can wait at most 24 hours (most jobs are completed in a few hours).</p>"
    },
    {
      "location": "quickstart/#example-use-cases",
      "title": "Example use-cases",
      "text": "<p>The range of use-cases you can tackle effortlessly with batchling is extremely wide, here are a few examples:</p> <ul> <li>Embedding text chunks for your RAG application overnight</li> <li>Running large-scale classification with your favourite GenAI provider and/or framework</li> <li>Run any GenAI evaluation pipeline</li> <li>Generate huge volume of synthtetic data at half the cost</li> <li>Transcribe or translate hours of audio in bulk</li> </ul> <p>Things you might not want to run with batchling (yes, there are some..):</p> <ul> <li>User-facing applications e.g. chatbots, you typically want fast answers</li> <li>A whole agentic loop with tons of calls</li> <li>Full AI workflows with a lot of sequential calls (each additional step will defer results by an additional 24 hours at worst)</li> </ul>"
    },
    {
      "location": "quickstart/#installation",
      "title": "Installation",
      "text": "<p>batchling is available on PyPI as <code>batchling</code>, install using either <code>pip</code> or <code>uv</code>:</p> uvpip <pre><code>uv add batchling\n</code></pre> <pre><code>pip install batchling\n</code></pre>"
    },
    {
      "location": "quickstart/#hello-world-example",
      "title": "Hello World Example",
      "text": "<p>batchling integrates smoothly with any async function doing GenAI calls or within a whole async script that you'd run with <code>asyncio</code>.</p> <p>Let's suppose we have an existing script <code>generate.py</code> that uses the OpenAI client to make two parallel calls using <code>asyncio.gather</code>:</p> <pre><code>import asyncio\nimport os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build OpenAI requests.\n    \"\"\"\n    client = AsyncOpenAI(api_key=os.getenv(key=\"OPENAI_API_KEY\"))\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    return [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the OpenAI example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    for response in responses:\n        content = response.output[-1].content # skip reasoning output, get straight to the answer\n        print(f\"{response.model} answer: {content[0].text}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> CLIPython SDK <p>For you to switch this async execution to a batched inference one, you just have to run your script using the <code>batchling</code> CLI and targetting the main function ran by <code>asyncio</code>:</p> <pre><code>batchling generate.py:main\n</code></pre> <p>To selectively batchify certain pieces of your code execution, you can rely on the <code>batchify</code> function, which exposes an async context manager.</p> <p>First, add this import at the top of your file:</p> <pre><code>+ from batchling import batchify\n</code></pre> <p>Then, let's modify our async function <code>main</code> to wrap the <code>asyncio.gather</code> call into the <code>batchify</code> async context manager:</p> <pre><code> async def main() -&gt; None:\n     \"\"\"Run the OpenAI example.\"\"\"\n     tasks = await build_tasks()\n-    responses = await asyncio.gather(*tasks)\n+    with batchify():\n+        responses = await asyncio.gather(*tasks)\n     for response in responses:\n         content = response.output[-1].content # skip reasoning output, get straight to the answer\n         print(f\"{response.model} answer: {content[0].text}\")\n</code></pre> <p>Output:</p> <pre><code>gpt-4o-mini-2024-07-18 answer: The best French painter is often considered to be Claude Monet, a leading figure in the Impressionist movement.\ngpt-5-nano-2025-08-07 answer: There isn\u2019t a universal \u201cbest\u201d French painter, but Claude Monet is widely regarded as one of the greatest.\n</code></pre> <p>You can run the script and see for yourself, normally small batches like that should run under 5-10 minutes at most.</p>"
    },
    {
      "location": "quickstart/#next-steps",
      "title": "Next Steps",
      "text": "<p>Now that you've seen how <code>batchling</code> can be used and want to learn more about it, you can header over to the following sections of the documentation:</p> <ul> <li> <p>Learn how to control batching behavior through batchify parameters</p> </li> <li> <p>Learn more about the Python SDK usage</p> </li> <li> <p>Learn more about CLI usage</p> </li> <li> <p>Learn about supported Providers &amp; Frameworks</p> </li> <li> <p>Browse batchling in-depth use-cases exploration</p> </li> </ul>"
    },
    {
      "location": "use-cases/",
      "title": "Use-Cases",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "frameworks/langchain/",
      "title": "LangChain",
      "text": "<p><code>batchling</code> was tested with LangChain using this example:</p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom langchain.agents import create_agent\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build LangChain requests.\n\n    Returns\n    -------\n    list[Awaitable[Any]]\n        Concurrent requests for batchling execution.\n    \"\"\"\n    agent = create_agent(\n        model=\"openai:gpt-4.1-mini\",\n    )\n    return [\n        agent.ainvoke(\n            input={\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"What is the best French painter?\"},\n                ]\n            }\n        ),\n        agent.ainvoke(\n            input={\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Where does 'hello world' come from?\"},\n                ]\n            }\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the LangChain example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    print(responses)\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    },
    {
      "location": "frameworks/pydantic_ai/",
      "title": "Pydantic AI",
      "text": "<p><code>batchling</code> was tested with Pydantic AI using this example:</p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom pydantic_ai import Agent\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build pydantic-ai requests.\n\n    Returns\n    -------\n    list[Awaitable[Any]]\n        Concurrent requests for batchling execution.\n    \"\"\"\n    agent = Agent(\n        model=\"openai:gpt-5-nano\",\n        tools=[],\n    )\n    return [\n        agent.run(user_prompt=\"What is the best French painter?\"),\n        agent.run(user_prompt=\"Where does 'hello world' come from?\"),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the pydantic-ai example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    print(responses)\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    },
    {
      "location": "providers/anthropic/",
      "title": "Anthropic",
      "text": ""
    },
    {
      "location": "providers/anthropic/#supported-endpoints",
      "title": "Supported endpoints",
      "text": "<ul> <li><code>/v1/messages</code></li> </ul>"
    },
    {
      "location": "providers/anthropic/#example-code",
      "title": "Example code",
      "text": "<pre><code>#!/usr/bin/env python3\nimport asyncio\nimport os\nimport typing as t\n\nfrom anthropic import AsyncAnthropic\nfrom dotenv import load_dotenv\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build Anthropic requests.\n\n    Returns\n    -------\n    list[Awaitable[Any]]\n        Concurrent requests for batchling execution.\n    \"\"\"\n    client = AsyncAnthropic(api_key=os.getenv(key=\"ANTHROPIC_API_KEY\"))\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    return [\n        client.messages.create(\n            max_tokens=1024,\n            messages=t.cast(t.Any, messages),\n            model=\"claude-haiku-4-5\",\n        ),\n        client.messages.create(\n            max_tokens=1024,\n            messages=t.cast(t.Any, messages),\n            model=\"claude-3-5-haiku-latest\",\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the Anthropic example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    print(responses)\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    },
    {
      "location": "providers/doubleword/",
      "title": "Doubleword",
      "text": ""
    },
    {
      "location": "providers/doubleword/#supported-endpoints",
      "title": "Supported endpoints",
      "text": "<ul> <li><code>/v1/chat/completions</code></li> <li><code>/v1/embeddings</code></li> <li><code>/v1/moderations</code></li> <li><code>/v1/completions</code></li> </ul>"
    },
    {
      "location": "providers/doubleword/#example-code",
      "title": "Example code",
      "text": "<ul> <li>No example file found in <code>examples/providers</code>.</li> </ul>"
    },
    {
      "location": "providers/gemini/",
      "title": "Gemini",
      "text": ""
    },
    {
      "location": "providers/gemini/#supported-endpoints",
      "title": "Supported endpoints",
      "text": "<ul> <li><code>/v1beta/models/{model}:generateContent</code></li> </ul>"
    },
    {
      "location": "providers/gemini/#example-code",
      "title": "Example code",
      "text": "<pre><code>#!/usr/bin/env python3\nimport asyncio\nimport base64\nimport os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom google import genai\nfrom pydantic import BaseModel, Field\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nclass ImageAnalysis(BaseModel):\n    \"\"\"Structured image-analysis response example.\n\n    Attributes\n    ----------\n    name : str\n        Name of the image content.\n    fun_fact : str\n        Short fun fact about the image.\n    \"\"\"\n\n    name: str = Field(description=\"the name of the image\")\n    fun_fact: str = Field(description=\"a fun fact about the image\")\n\n\ndef encode_image(image_path: str) -&gt; str:\n    \"\"\"Encode an image file as base64.\n\n    Parameters\n    ----------\n    image_path : str\n        Absolute or relative path to an image.\n\n    Returns\n    -------\n    str\n        Base64-encoded content.\n    \"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build Gemini requests.\n\n    Returns\n    -------\n    list[asyncio.Future]\n        Concurrent requests for batchling execution.\n    \"\"\"\n    client = genai.Client(api_key=os.getenv(key=\"GEMINI_API_KEY\")).aio\n    contents = \"Who is the best French painter? Answer in one short sentence.\"\n    return [\n        client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=contents,\n        ),\n        client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=contents,\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the Gemini example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    print(responses)\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    },
    {
      "location": "providers/groq/",
      "title": "Groq",
      "text": ""
    },
    {
      "location": "providers/groq/#supported-endpoints",
      "title": "Supported endpoints",
      "text": "<ul> <li><code>/openai/v1/chat/completions</code></li> <li><code>/openai/v1/audio/transcriptions</code></li> <li><code>/openai/v1/audio/translations</code></li> </ul>"
    },
    {
      "location": "providers/groq/#example-code",
      "title": "Example code",
      "text": "<pre><code>#!/usr/bin/env python3\nimport asyncio\nimport os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom groq import AsyncGroq\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build Groq requests.\n\n    Returns\n    -------\n    list[Awaitable[Any]]\n        Concurrent requests for batchling execution.\n    \"\"\"\n    client = AsyncGroq(api_key=os.getenv(key=\"GROQ_API_KEY\"))\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    return [\n        client.chat.completions.create(\n            model=\"llama-3.1-8b-instant\",\n            messages=t.cast(t.Any, messages),\n        ),\n        client.chat.completions.create(\n            model=\"openai/gpt-oss-20b\",\n            messages=t.cast(t.Any, messages),\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the Groq example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    print(responses)\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    },
    {
      "location": "providers/mistral/",
      "title": "Mistral",
      "text": ""
    },
    {
      "location": "providers/mistral/#supported-endpoints",
      "title": "Supported endpoints",
      "text": "<ul> <li><code>/v1/chat/completions</code></li> <li><code>/v1/fim/completions</code></li> <li><code>/v1/embeddings</code></li> <li><code>/v1/moderations</code></li> <li><code>/v1/chat/moderations/v1/ocr</code></li> <li><code>/v1/classifications</code></li> <li><code>/v1/conversations/v1/audio/transcriptions</code></li> </ul>"
    },
    {
      "location": "providers/mistral/#example-code",
      "title": "Example code",
      "text": "<pre><code>#!/usr/bin/env python3\nimport asyncio\nimport os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom mistralai import Mistral\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build Mistral requests.\n\n    Returns\n    -------\n    list[Awaitable[Any]]\n        Concurrent requests for batchling execution.\n    \"\"\"\n    client = Mistral(api_key=os.getenv(key=\"MISTRAL_API_KEY\"))\n    messages: list[dict[str, str]] = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    return [\n        client.chat.complete_async(\n            model=\"mistral-medium-2505\",\n            messages=t.cast(t.Any, messages),\n            stream=False,\n            response_format={\"type\": \"text\"},\n        ),\n        client.chat.complete_async(\n            model=\"mistral-small-2506\",\n            messages=t.cast(t.Any, messages),\n            stream=False,\n            response_format={\"type\": \"text\"},\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the Mistral example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    print(responses)\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    },
    {
      "location": "providers/openai/",
      "title": "OpenAI",
      "text": ""
    },
    {
      "location": "providers/openai/#supported-endpoints",
      "title": "Supported endpoints",
      "text": "<ul> <li><code>/v1/responses</code></li> <li><code>/v1/chat/completions</code></li> <li><code>/v1/embeddings</code></li> <li><code>/v1/completions</code></li> <li><code>/v1/moderations</code></li> <li><code>/v1/images/generations</code></li> <li><code>/v1/images/edits</code></li> </ul>"
    },
    {
      "location": "providers/openai/#example-code",
      "title": "Example code",
      "text": "<pre><code>#!/usr/bin/env python3\nimport asyncio\nimport os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list:\n    \"\"\"Build OpenAI requests.\"\"\"\n    client = AsyncOpenAI(api_key=os.getenv(key=\"OPENAI_API_KEY\"))\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    return [\n        client.responses.create(\n            input=t.cast(t.Any, messages),\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=t.cast(t.Any, messages),\n            model=\"gpt-5-nano\",\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the OpenAI example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    for response in responses:\n        content = response.output[-1].content  # skip reasoning output, get straight to the answer\n        print(f\"{response.model} answer: {content[0].text}\")\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    },
    {
      "location": "providers/together/",
      "title": "Together",
      "text": ""
    },
    {
      "location": "providers/together/#supported-endpoints",
      "title": "Supported endpoints",
      "text": "<ul> <li><code>/v1/chat/completions</code></li> <li><code>/v1/audio/transcriptions</code></li> </ul>"
    },
    {
      "location": "providers/together/#example-code",
      "title": "Example code",
      "text": "<pre><code>#!/usr/bin/env python3\nimport asyncio\nimport os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom together import AsyncTogether\n\nfrom batchling import batchify\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build Together AI requests.\n\n    Returns\n    -------\n    list[Awaitable[Any]]\n        Concurrent requests for batchling execution.\n    \"\"\"\n    client = AsyncTogether(api_key=os.getenv(key=\"TOGETHER_API_KEY\"))\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    return [\n        client.chat.completions.create(\n            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n            messages=t.cast(t.Any, messages),\n        ),\n        client.chat.completions.create(\n            model=\"google/gemma-3n-E4B-it\",\n            messages=t.cast(t.Any, messages),\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the Together AI example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    print(responses)\n\n\nasync def run_with_batchify() -&gt; None:\n    \"\"\"Run `main` inside `batchify` for direct script execution.\"\"\"\n    async with batchify():\n        await main()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_with_batchify())\n</code></pre>"
    }
  ]
}
