{
  "config": {
    "lang": [
      "en"
    ],
    "separator": "[\\s\\-]+",
    "pipeline": [
      "stopWordFilter"
    ],
    "fields": {
      "title": {
        "boost": 1000.0
      },
      "text": {
        "boost": 1.0
      },
      "tags": {
        "boost": 1000000.0
      }
    }
  },
  "docs": [
    {
      "location": "",
      "title": "batchling",
      "text": "<p>Batch API calls from familiar clients.</p> <p>batchling intercepts supported requests, groups them by provider, and   returns normal responses while reducing cost and latency overhead.</p> Quickstart Install and run the first async batching flow in minutes. Configuration Tune batch sizes, windows, and dry-run behavior safely. Providers Check supported adapters and compatibility notes."
    },
    {
      "location": "#batchling_1",
      "title": "batchling",
      "text": "<p> Save 50% off GenAI costs in two lines of code </p> <p> <p>batchling is a frictionless, batteries-included plugin to convert any GenAI async function or script into half-cost deferred jobs.</p> <p>Key features:</p> <ul> <li>Simplicity: a simple 2-liner gets you 50% off your GenAI bill instantly.</li> <li>Transparent: Your code remains the same, no added behaviors. Track sent batches easily.</li> <li>Global: Integrates with most providers and all frameworks.</li> <li>Safe: Get a complete breakdown of your cost savings before launching a single batch.</li> <li>Lightweight: Very few dependencies.</li> </ul> What's the catch? <p>The catch is the Batch API!</p> <p>Batch APIs enable you to process large volumes of requests asynchronously (usually at 50% lower cost compared to real-time API calls). It's perfect for workloads that don't need immediate responses such as:</p> <ul> <li>Running mass offline evaluations</li> <li>Classifying large datasets</li> <li>Generating large-scale embeddings</li> <li>Offline summarization</li> <li>Synthetic data generation</li> <li>Structured data extraction (e.g. OCR)</li> <li>Audio transcriptions/translations at scale</li> </ul> <p>Compared to using standard endpoints directly, Batch API offers:</p> <ul> <li>Better cost efficiency: usually 50% cost discount compared to synchronous APIs</li> <li>Higher rate limits: Substantially more headroom with separate rate limit pools</li> <li>Large-scale support: Process thousands of requests per batch</li> <li>Flexible completion: Best-effort completion within 24 hours with progress tracking, batches usually complete within an hour.</li> </ul>"
    },
    {
      "location": "#installation",
      "title": "Installation",
      "text": "<pre><code>pip install batchling\n</code></pre>"
    },
    {
      "location": "#get-started",
      "title": "Get Started",
      "text": ""
    },
    {
      "location": "#using-the-async-context-manager-recommended",
      "title": "Using the async context manager (recommended)",
      "text": "<pre><code>import asyncio\nfrom batchling import batchify\nfrom openai import AsyncOpenAI\n\nasync def generate():\n    client = AsyncOpenAI()\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    tasks = [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n    with batchify(): # Runs your tasks as batches, save 50%\n        responses = await asyncio.gather(*tasks)\n</code></pre>"
    },
    {
      "location": "#using-the-cli-wrapper",
      "title": "Using the CLI wrapper",
      "text": "<p>Create a file <code>main.py</code> with:</p> <pre><code>import asyncio\nfrom openai import AsyncOpenAI\n\nasync def generate():\n    client = AsyncOpenAI()\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    tasks = [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n    responses = await asyncio.gather(*tasks)\n</code></pre> <p>Run your function in batch mode:</p> <pre><code>batchling main.py:generate\n</code></pre>"
    },
    {
      "location": "#how-it-works",
      "title": "How it works",
      "text": ""
    },
    {
      "location": "#request-interception",
      "title": "Request interception",
      "text": "<p>batchling patches <code>httpx</code> and <code>aiohttp</code>, the two main async request utilities to capture certain requests based on batchable endpoint detection for GenAI providers exposing a batch API.</p>"
    },
    {
      "location": "#batch-management",
      "title": "Batch Management",
      "text": "<p>Relevant requests are routed to a batch manager, which orchestrates batch queues. Batch queues behaviors are parametrized by user-defined parameters <code>batch_size</code> and <code>batch_window_size</code>. Each batch queue is associated with a unique <code>(provider, endpoint, model)</code> key.</p>"
    },
    {
      "location": "#requests-repurposing",
      "title": "Requests repurposing",
      "text": "<p>Once batches are ready to go, the received requests are repurposed into batch-compatible requests and sent to the provider. Batches are automatically polled every <code>batch_poll_interval_seconds</code> seconds. Once all batches have been processed, the data is returned through the <code>httpx</code> or <code>aiohttp</code> patch like a regular request would have been, letting the rest of the script execute.</p>"
    },
    {
      "location": "#supported-providers",
      "title": "Supported providers",
      "text": "Name Batch API Docs URL OpenAI https://platform.openai.com/docs/guides/batch Anthropic https://docs.anthropic.com/en/docs/build-with-claude/batch-processing Gemini https://ai.google.dev/gemini-api/docs/batch-mode Groq https://console.groq.com/docs/batch Mistral https://docs.mistral.ai/capabilities/batch/ Together AI https://docs.together.ai/docs/batch-inference Doubleword https://docs.doubleword.ai/batches/getting-started-with-batched-api"
    },
    {
      "location": "configuration/",
      "title": "Configuration",
      "text": "<p>batchling exposes four user-facing controls.</p>"
    },
    {
      "location": "configuration/#batch_size",
      "title": "<code>batch_size</code>",
      "text": "<ul> <li>Type: <code>int</code></li> <li>Default: <code>50</code></li> <li>Meaning: submit when this many queued requests exist for a queue key.</li> </ul>"
    },
    {
      "location": "configuration/#batch_window_seconds",
      "title": "<code>batch_window_seconds</code>",
      "text": "<ul> <li>Type: <code>float</code></li> <li>Default: <code>2.0</code></li> <li>Meaning: submit when this time elapses, even if <code>batch_size</code> is not reached.</li> </ul>"
    },
    {
      "location": "configuration/#batch_poll_interval_seconds",
      "title": "<code>batch_poll_interval_seconds</code>",
      "text": "<ul> <li>Type: <code>float</code></li> <li>Default: <code>10.0</code></li> <li>Meaning: polling interval for active provider batches.</li> </ul>"
    },
    {
      "location": "configuration/#dry_run",
      "title": "<code>dry_run</code>",
      "text": "<ul> <li>Type: <code>bool</code></li> <li>Default: <code>False</code></li> <li>Meaning: intercept and group requests without sending provider batches.</li> </ul>"
    },
    {
      "location": "configuration/#queue-semantics",
      "title": "Queue semantics",
      "text": "<p>Batch queues are partitioned by strict key:</p> <ul> <li><code>provider</code></li> <li><code>endpoint</code></li> <li><code>model</code></li> </ul> <p>This means thresholds apply per <code>(provider, endpoint, model)</code> group, not globally.</p>"
    },
    {
      "location": "configuration/#example-tuning",
      "title": "Example tuning",
      "text": "<pre><code>async with batchify(\n    batch_size=200,\n    batch_window_seconds=5.0,\n    batch_poll_interval_seconds=15.0,\n    dry_run=False,\n):\n    ...\n</code></pre>"
    },
    {
      "location": "getting-started/",
      "title": "Getting Started",
      "text": ""
    },
    {
      "location": "getting-started/#install",
      "title": "Install",
      "text": "<pre><code>pip install batchling\n</code></pre>"
    },
    {
      "location": "getting-started/#minimal-async-usage-recommended",
      "title": "Minimal async usage (recommended)",
      "text": "<pre><code>import asyncio\nfrom batchling import batchify\nfrom openai import AsyncOpenAI\n\n\nasync def generate() -&gt; None:\n    client = AsyncOpenAI()\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"Classify this text in one label: 'Batch APIs reduce cost for deferred jobs.'\",\n        }\n    ]\n\n    async with batchify():\n        responses = await asyncio.gather(\n            client.responses.create(\n                model=\"gpt-4o-mini\",\n                input=messages,\n            ),\n            client.responses.create(\n                model=\"gpt-4o-mini\",\n                input=messages,\n            ),\n        )\n\n    print(responses)\n\n\nasyncio.run(generate())\n</code></pre>"
    },
    {
      "location": "getting-started/#what-to-expect-at-runtime",
      "title": "What to expect at runtime",
      "text": "<ul> <li>batchling installs request hooks and watches supported HTTP requests.</li> <li>Supported requests are grouped by <code>(provider, endpoint, model)</code>.</li> <li>A batch is submitted when either queue size or window threshold is hit.</li> <li>Batch results are polled and mapped back to each original request.</li> </ul>"
    },
    {
      "location": "getting-started/#cli-wrapper-option",
      "title": "CLI wrapper option",
      "text": "<p>If your workload already lives in an async function inside a script, use the CLI wrapper:</p> <pre><code>batchling path/to/script.py:generate --batch-size 100 --batch-window-seconds 3.0\n</code></pre> <p>The target must be an <code>async def</code> callable.</p>"
    },
    {
      "location": "providers/",
      "title": "Providers",
      "text": "<p>batchling currently supports these provider adapters.</p> Provider Batch docs OpenAI https://platform.openai.com/docs/guides/batch Anthropic https://docs.anthropic.com/en/docs/build-with-claude/batch-processing Gemini https://ai.google.dev/gemini-api/docs/batch-mode Groq https://console.groq.com/docs/batch Mistral https://docs.mistral.ai/capabilities/batch/ Together AI https://docs.together.ai/docs/batch-inference Doubleword https://docs.doubleword.ai/batches/getting-started-with-batched-api"
    },
    {
      "location": "providers/#compatibility-notes",
      "title": "Compatibility notes",
      "text": "<ul> <li>Matching is based on provider hostname plus supported batchable endpoints.</li> <li>Requests that are not recognized as batchable are not routed through the batch engine.</li> <li>Provider response decoding is normalized back into request-level responses.</li> </ul> <p>Implementation details for adapters are kept in AGENTS-oriented internal docs.</p>"
    },
    {
      "location": "troubleshooting/",
      "title": "Troubleshooting",
      "text": ""
    },
    {
      "location": "troubleshooting/#requests-are-not-being-batched",
      "title": "Requests are not being batched",
      "text": "<p>Check:</p> <ul> <li>The request targets a supported provider hostname.</li> <li>The endpoint is batchable for that provider.</li> <li>The call runs inside <code>batchify()</code> scope (or via <code>batchling</code> CLI wrapper).</li> </ul>"
    },
    {
      "location": "troubleshooting/#script-fails-in-cli-mode",
      "title": "Script fails in CLI mode",
      "text": "<p>Check:</p> <ul> <li>Target format is <code>path/to/script.py:function_name</code>.</li> <li>The function exists in that script.</li> <li>The function is <code>async def</code>.</li> </ul>"
    },
    {
      "location": "troubleshooting/#batch-completion-feels-slow",
      "title": "Batch completion feels slow",
      "text": "<ul> <li>Increase <code>batch_size</code> only if you have enough traffic to fill queues quickly.</li> <li>Reduce <code>batch_window_seconds</code> for lower queue wait time.</li> <li>Reduce <code>batch_poll_interval_seconds</code> to check finished batches more frequently.</li> </ul>"
    },
    {
      "location": "troubleshooting/#validate-setup-safely",
      "title": "Validate setup safely",
      "text": "<p>Run with dry-run to verify interception and queueing before real provider submissions:</p> <pre><code>batchling script.py:main --dry-run\n</code></pre>"
    },
    {
      "location": "usage-patterns/",
      "title": "Usage Patterns",
      "text": ""
    },
    {
      "location": "usage-patterns/#choose-the-right-execution-style",
      "title": "Choose the right execution style",
      "text": ""
    },
    {
      "location": "usage-patterns/#use-async-with-batchify-when",
      "title": "Use <code>async with batchify(...)</code> when",
      "text": "<ul> <li>You control the application code.</li> <li>You want explicit batching scope around a block of async calls.</li> <li>You need local tuning per workflow.</li> </ul>"
    },
    {
      "location": "usage-patterns/#use-batchling-scriptpyfunction-when",
      "title": "Use <code>batchling script.py:function</code> when",
      "text": "<ul> <li>You want to run an existing async script with minimal edits.</li> <li>You need to pass function args from CLI.</li> <li>You want to automate batched script runs from CI or schedulers.</li> </ul>"
    },
    {
      "location": "usage-patterns/#forwarding-function-arguments-in-cli-mode",
      "title": "Forwarding function arguments in CLI mode",
      "text": "<pre><code>batchling jobs/run_eval.py:main dataset_a --limit 200 --dry-run\n</code></pre> <ul> <li>Positional tokens are forwarded as positional args.</li> <li><code>--name value</code> or <code>--name=value</code> are forwarded as keyword args.</li> <li>Standalone flags like <code>--dry-run</code> are forwarded as boolean <code>True</code> keyword args.</li> </ul>"
    },
    {
      "location": "usage-patterns/#dry-run-pattern",
      "title": "Dry run pattern",
      "text": "<p>Use dry-run before production launch:</p> <pre><code>async with batchify(dry_run=True):\n    ...\n</code></pre> <p>Dry-run still intercepts and queues supported requests, but it skips provider submission and polling.</p>"
    }
  ]
}
