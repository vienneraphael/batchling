{
  "config": {
    "lang": [
      "en"
    ],
    "separator": "[\\s\\-]+",
    "pipeline": [
      "stopWordFilter"
    ],
    "fields": {
      "title": {
        "boost": 1000.0
      },
      "text": {
        "boost": 1.0
      },
      "tags": {
        "boost": 1000000.0
      }
    }
  },
  "docs": [
    {
      "location": "",
      "title": "batchling",
      "text": "<p>Save 50% off GenAI costs in two lines of code.</p> <p>batchling repurposes incoming GenAI requests to batch format, gets results from supported Batch APIs and   returns responses seamlessly in your code execution, reducing GenAI costs at scale with zero friction.</p> Quickstart Install and run the first async batching flow in minutes. Frameworks &amp; Providers See the list of supported frameworks and providers Use-Cases Browse through real-life use-cases of batchling Advanced Features Learn about advanced features: caching, dry run, deferred mode.. <p> Save 50% off GenAI costs in two lines of code </p> <p> <p>batchling is a frictionless, batteries-included plugin to convert any GenAI async function or script into half-cost deferred jobs.</p> <p>Key features:</p> <ul> <li>Simplicity: a simple 2-liner gets you 50% off your GenAI bill instantly.</li> <li>Transparent: Your code remains the same, no added behaviors. Track sent batches easily.</li> <li>Global: Integrates with most providers and all frameworks.</li> <li>Safe: Get a complete breakdown of your cost savings before launching a single batch.</li> <li>Lightweight: Very few dependencies.</li> </ul> What's the catch? <p>The batch is the catch!</p> <p>Batch APIs enable you to process large volumes of requests asynchronously (usually at 50% lower cost compared to real-time API calls). It's perfect for workloads that don't need immediate responses such as:</p> <ul> <li>Running mass offline evaluations</li> <li>Classifying large datasets</li> <li>Generating large-scale embeddings</li> <li>Offline summarization</li> <li>Synthetic data generation</li> <li>Structured data extraction (e.g. OCR)</li> <li>Audio transcriptions/translations at scale</li> </ul> <p>Compared to using standard endpoints directly, Batch API offers:</p> <ul> <li>Better cost efficiency: usually 50% cost discount compared to synchronous APIs</li> <li>Higher rate limits: Substantially more headroom with separate rate limit pools</li> <li>Large-scale support: Process thousands of requests per batch</li> <li>Flexible completion: Best-effort completion within 24 hours with progress tracking, batches usually complete within an hour.</li> </ul>"
    },
    {
      "location": "#installation",
      "title": "Installation",
      "text": "<pre><code>pip install batchling\n</code></pre>"
    },
    {
      "location": "#get-started",
      "title": "Get Started",
      "text": ""
    },
    {
      "location": "#using-the-async-context-manager-recommended",
      "title": "Using the async context manager (recommended)",
      "text": "<pre><code>import asyncio\nfrom batchling import batchify\nfrom openai import AsyncOpenAI\n\nasync def generate():\n    client = AsyncOpenAI()\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    tasks = [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n    with batchify(): # Runs your tasks as batches, save 50%\n        responses = await asyncio.gather(*tasks)\n</code></pre>"
    },
    {
      "location": "#using-the-cli-wrapper",
      "title": "Using the CLI wrapper",
      "text": "<p>Create a file <code>main.py</code> with:</p> <pre><code>import asyncio\nfrom openai import AsyncOpenAI\n\nasync def generate():\n    client = AsyncOpenAI()\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    tasks = [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n    responses = await asyncio.gather(*tasks)\n</code></pre> <p>Run your function in batch mode:</p> <pre><code>batchling main.py:generate\n</code></pre>"
    },
    {
      "location": "#how-it-works",
      "title": "How it works",
      "text": ""
    },
    {
      "location": "#request-interception",
      "title": "Request interception",
      "text": "<p>batchling patches <code>httpx</code> and <code>aiohttp</code>, the two main async request utilities to capture certain requests based on batchable endpoint detection for GenAI providers exposing a batch API.</p>"
    },
    {
      "location": "#batch-management",
      "title": "Batch Management",
      "text": "<p>Relevant requests are routed to a batch manager, which orchestrates batch queues. Batch queues behaviors are parametrized by user-defined parameters <code>batch_size</code> and <code>batch_window_size</code>. Each batch queue is associated with a unique <code>(provider, endpoint, model)</code> key.</p>"
    },
    {
      "location": "#requests-repurposing",
      "title": "Requests repurposing",
      "text": "<p>Once batches are ready to go, the received requests are repurposed into batch-compatible requests and sent to the provider. Batches are automatically polled every <code>batch_poll_interval_seconds</code> seconds. Once all batches have been processed, the data is returned through the <code>httpx</code> or <code>aiohttp</code> patch like a regular request would have been, letting the rest of the script execute.</p>"
    },
    {
      "location": "#request-cache-and-deferred-mode",
      "title": "Request cache and deferred mode",
      "text": "<ul> <li>Request cache is enabled by default and can be disabled with <code>--no-cache</code> or <code>cache=False</code>.</li> <li>Cache entries are cleaned with a one-month retention window.</li> <li>Deferred mode (<code>--deferred</code> or <code>deferred=True</code>) can stop runtime when only polling remains idle.</li> <li>CLI mode exits successfully with a deferred message; Python usage raises <code>DeferredExit</code>.</li> </ul>"
    },
    {
      "location": "#supported-providers",
      "title": "Supported providers",
      "text": "Name Batch API Docs URL OpenAI https://platform.openai.com/docs/guides/batch Anthropic https://docs.anthropic.com/en/docs/build-with-claude/batch-processing Gemini https://ai.google.dev/gemini-api/docs/batch-mode Groq https://console.groq.com/docs/batch Mistral https://docs.mistral.ai/capabilities/batch/ Together AI https://docs.together.ai/docs/batch-inference Doubleword https://docs.doubleword.ai/batches/getting-started-with-batched-api"
    },
    {
      "location": "advanced-features/",
      "title": "Advanced Features",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "cache/",
      "title": "Cache",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "cli/",
      "title": "CLI",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "deferred-mode/",
      "title": "Deferred Mode",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "dry-run/",
      "title": "Dry Run",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "frameworks/",
      "title": "Frameworks",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "overview/",
      "title": "Overview",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "providers/",
      "title": "Providers",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "python-sdk/",
      "title": "Python SDK",
      "text": "<p>Placeholder content.</p>"
    },
    {
      "location": "quickstart/",
      "title": "Quickstart",
      "text": "<p>batchling is a powerful python library that lets you transform any async GenAI workflow to batched inference, saving you money in exchange of deferred execution (24 hours bound). You typically want to use batched inference when running any process that does not require immediate response and can wait at most 24 hours (most jobs are completed in a few hours).</p>"
    },
    {
      "location": "quickstart/#example-use-cases",
      "title": "Example use-cases",
      "text": "<p>The range of use-cases you can tackle effortlessly with batchling is extremely wide, here are a few examples:</p> <ul> <li>Embedding text chunks for your RAG application overnight</li> <li>Running large-scale classification with your favourite GenAI provider and/or framework</li> <li>Run any GenAI evaluation pipeline</li> <li>Generate huge volume of synthtetic data at half the cost</li> <li>Transcribe or translate hours of audio in bulk</li> </ul> <p>Things you might not want to run with batchling (yes, there are some..):</p> <ul> <li>User-facing applications e.g. chatbots, you typically want fast answers</li> <li>A whole agentic loop with tons of calls</li> <li>Full AI workflows with a lot of sequential calls (each additional step will defer results by an additional 24 hours at worst)</li> </ul>"
    },
    {
      "location": "quickstart/#installation",
      "title": "Installation",
      "text": "<p>batchling is available on PyPI as <code>batchling</code>, install using either <code>pip</code> or <code>uv</code>:</p> uvpip <pre><code>uv add batchling\n</code></pre> <pre><code>pip install batchling\n</code></pre>"
    },
    {
      "location": "quickstart/#hello-world-example",
      "title": "Hello World Example",
      "text": "<p>batchling integrates smoothly with any async function doing GenAI calls or within a whole async script that you'd run with <code>asyncio</code>.</p> <p>Let's suppose we have an existing script <code>generate.py</code> that uses the OpenAI client to make two parallel calls using <code>asyncio.gather</code>:</p> <pre><code>import asyncio\nimport os\nimport typing as t\n\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\n\nload_dotenv()\n\n\nasync def build_tasks() -&gt; list[t.Awaitable[t.Any]]:\n    \"\"\"Build OpenAI requests.\n    \"\"\"\n    client = AsyncOpenAI(api_key=os.getenv(key=\"OPENAI_API_KEY\"))\n    messages = [\n        {\n            \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n            \"role\": \"user\",\n        },\n    ]\n    return [\n        client.responses.create(\n            input=messages,\n            model=\"gpt-4o-mini\",\n        ),\n        client.responses.create(\n            input=messages,\n            model=\"gpt-5-nano\",\n        ),\n    ]\n\n\nasync def main() -&gt; None:\n    \"\"\"Run the OpenAI example.\"\"\"\n    tasks = await build_tasks()\n    responses = await asyncio.gather(*tasks)\n    for response in responses:\n        content = response.output[-1].content # skip reasoning output, get straight to the answer\n        print(f\"{response.model} answer: {content[0].text}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> CLIPython SDK <p>For you to switch this async execution to a batched inference one, you just have to run your script using the <code>batchling</code> CLI and targetting the main function ran by <code>asyncio</code>:</p> <pre><code>batchling generate.py:main\n</code></pre> <p>To selectively batchify certain pieces of your code execution, you can rely on the <code>batchify</code> function, which exposes an async context manager.</p> <p>First, add this import at the top of your file:</p> <pre><code>+ from batchling import batchify\n</code></pre> <p>Then, let's modify our async function <code>main</code> to wrap the <code>asyncio.gather</code> call into the <code>batchify</code> async context manager:</p> <pre><code> async def main() -&gt; None:\n     \"\"\"Run the OpenAI example.\"\"\"\n     tasks = await build_tasks()\n-    responses = await asyncio.gather(*tasks)\n+    with batchify():\n+        responses = await asyncio.gather(*tasks)\n     for response in responses:\n         content = response.output[-1].content # skip reasoning output, get straight to the answer\n         print(f\"{response.model} answer: {content[0].text}\")\n</code></pre> <p>Output:</p> <pre><code>gpt-4o-mini-2024-07-18 answer: The best French painter is often considered to be Claude Monet, a leading figure in the Impressionist movement.\ngpt-5-nano-2025-08-07 answer: There isn\u2019t a universal \u201cbest\u201d French painter, but Claude Monet is widely regarded as one of the greatest.\n</code></pre> <p>You can run the script and see for yourself, normally small batches like that should run under 5-10 minutes at most.</p>"
    },
    {
      "location": "use-cases/",
      "title": "Use-Cases",
      "text": "<p>Placeholder content.</p>"
    }
  ]
}
